# Product Data Explorer

A production-minded product exploration platform for browsing and discovering products from World of Books.

## ğŸ¯ Features

- **Smart Navigation**: Browse from high-level headings â†’ categories â†’ products â†’ detailed pages
- **Live Scraping**: Real-time, on-demand data fetching from World of Books
- **Intelligent Caching**: Avoid excessive scraping with TTL-based caching
- **Responsive Design**: Mobile-first, accessible UI with smooth transitions
- **Browsing History**: Track and persist user navigation paths
- **Product Details**: Complete information including reviews, ratings, and recommendations

## ğŸ—ï¸ Architecture

### Tech Stack

**Frontend:**
- Next.js 14+ (App Router)
- TypeScript
- Tailwind CSS
- SWR for data fetching
- Responsive & accessible design

**Backend:**
- NestJS (Node.js + TypeScript)
- PostgreSQL (production database)
- Crawlee + Playwright (web scraping)
- TypeORM (database ORM)
- Bull (job queue)
- Swagger/OpenAPI documentation

**Infrastructure:**
- Docker & Docker Compose
- GitHub Actions (CI/CD)
- Rate limiting & backoff strategies

### Project Structure

```
Data Explorer/
â”œâ”€â”€ frontend/          # Next.js application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/      # App router pages
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ lib/      # Utilities & API client
â”‚   â”‚   â””â”€â”€ types/
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ backend/           # NestJS application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ modules/  # Feature modules
â”‚   â”‚   â”œâ”€â”€ scraper/  # Crawlee scraping logic
â”‚   â”‚   â””â”€â”€ database/ # Database entities
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ and npm/yarn
- PostgreSQL 14+ (or use a managed service like Railway/Render)
- Redis 7+ (optional, for job queue)

### Local Setup

#### Backend Setup

```bash
cd backend

# Install dependencies
npm install

# Configure environment
cp .env.example .env
# Edit .env with your PostgreSQL credentials

# Run migrations
npm run migration:run

# Seed database (optional)
npm run seed

# Start development server
npm run start:dev

# Backend runs on http://localhost:3001
```

#### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Configure environment
cp .env.example .env
# Edit .env with backend API URL

# Start development server
npm run dev

# Frontend runs on http://localhost:3000
```

## ğŸ“¦ Environment Variables

### Backend (.env)

```env
NODE_ENV=development
PORT=3001

# Database
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_USER=postgres
DATABASE_PASSWORD=your_password
DATABASE_NAME=product_explorer

# Redis (optional, for caching)
REDIS_HOST=localhost
REDIS_PORT=6379

# Scraping
SCRAPE_CACHE_TTL=3600
SCRAPE_RATE_LIMIT=5
SCRAPE_DELAY_MS=2000
```

### Frontend (.env)

```env
NEXT_PUBLIC_API_URL=http://localhost:3001
```

## ğŸ—„ï¸ Database Schema

### Core Entities

- **navigation**: Top-level navigation headings
- **category**: Categories and subcategories (self-referential)
- **product**: Product listings with basic info
- **product_detail**: Extended product information
- **review**: User reviews and ratings
- **scrape_job**: Scraping job tracking
- **view_history**: User browsing history

### Relationships

```
navigation 1--* category
category 1--* category (parent-child)
category 1--* product
product 1--1 product_detail
product 1--* review
```

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
npm run test              # Unit tests
npm run test:e2e          # E2E tests
npm run test:cov          # Coverage

# Frontend tests
cd frontend
npm run test
npm run test:coverage
```

## ğŸ“š API Documentation

API documentation is available via Swagger UI when the backend is running:

**Local**: http://localhost:3001/api
**Production**: [Your deployed backend URL]/api

### Key Endpoints

```
GET  /api/navigation              # Get all navigation headings
GET  /api/categories              # Get categories
GET  /api/categories/:id          # Get category details
GET  /api/products                # Get products (paginated)
GET  /api/products/:id            # Get product details
POST /api/scrape/navigation       # Trigger navigation scrape
POST /api/scrape/category/:id     # Trigger category scrape
POST /api/scrape/product/:id      # Trigger product scrape
GET  /api/history                 # Get browsing history
POST /api/history                 # Save browsing history
```

## ğŸ›¡ï¸ Ethical Scraping

This application implements responsible scraping practices:

- âœ… Respects robots.txt
- âœ… Rate limiting with delays (2s between requests)
- âœ… Exponential backoff on failures
- âœ… Aggressive caching to minimize requests
- âœ… User-Agent identification
- âœ… Queue-based job processing

## ğŸš¢ Deployment

### Backend Deployment

The backend can be deployed to:
- Railway
- Render
- Heroku
- Fly.io
- DigitalOcean App Platform

Ensure environment variables are set in your deployment platform.

### Frontend Deployment

The frontend is optimized for Vercel deployment:

```bash
cd frontend
npm run build
# Deploy to Vercel, Netlify, or other platforms
```

## ğŸ”§ Design Decisions

### Why PostgreSQL?
- Strong relational model for navigation â†’ categories â†’ products
- ACID compliance for data integrity
- Excellent JSON support for flexible metadata
- Robust indexing and query performance

### Why Crawlee + Playwright?
- Modern scraping framework with queue management
- Headless browser support for JavaScript-rendered content
- Built-in retries and error handling
- Request throttling and deduplication

### Why SWR?
- Stale-while-revalidate strategy for optimal UX
- Built-in caching and request deduplication
- Optimistic UI updates
- Lightweight and performant

## ğŸ“ Development Workflow

### Adding a New Feature

1. **Backend**: Create module, service, controller, DTO
2. **Database**: Add/update entities, create migration
3. **Frontend**: Create page/component, add API integration
4. **Tests**: Write unit and integration tests
5. **Documentation**: Update README and Swagger docs

### Database Migrations

```bash
# Generate migration
npm run migration:generate -- -n MigrationName

# Run migrations
npm run migration:run

# Revert migration
npm run migration:revert
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

MIT License - feel free to use this project for your portfolio or learning.

## ğŸ”— Links

- **GitHub Repository**: [Your repo URL]
- **Live Frontend**: [Your deployed frontend URL]
- **Live Backend**: [Your deployed backend URL]
- **API Documentation**: [Your backend URL]/api

## ğŸ‘¨â€ğŸ’» Author

[Your Name]

## ğŸ™ Acknowledgments

- World of Books for product data
- NestJS and Next.js communities
- Crawlee team for the excellent scraping framework

---

**Built with â¤ï¸ for the full-stack engineering assessment**
